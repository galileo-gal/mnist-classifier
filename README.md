# MNIST Handwritten Digit Classifier


A production-grade machine learning pipeline for MNIST digit classification, built to demonstrate proper ML engineering practices from data loading to model evaluation.

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## ğŸ¯ Project Goals

This project is designed to learn and implement:
- **Neural networks from first principles** (backpropagation, gradient descent)
- **PyTorch fundamentals** (nn.Module, DataLoader, optimizers)
- **Production ML pipeline** (config-driven experiments, logging, checkpointing)
- **Proper evaluation methodology** (train/val/test splits, early stopping)
- **Model interpretability** (confusion matrices, failure analysis, filter visualization)

**Current Status:** Phase 2 Complete - Baseline FC Model (97.82% test accuracy)

---

## ğŸ“Š Results

| Model | Architecture | Test Accuracy | Parameters | Training Time |
|-------|-------------|---------------|------------|---------------|
| **FC (Production)** | 784â†’256â†’128â†’10 | **97.82%** | 235,146 | ~5 min (CPU) |
| FC (PyTorch - Legacy) | 784â†’256â†’128â†’10 | 98.16% | 235,146 | ~3 min (CPU, subset) |
| FC (From Scratch - Legacy) | 784â†’128â†’64â†’10 | 91.66% | - | ~10 min (Colab GPU) |

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/galileo-gal/mnist-classifier.git
cd mnist-classifier

# Create virtual environment
python -m venv .venv

# Activate virtual environment
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### Train a Model

```bash
# Train with baseline config
python scripts/train.py --config configs/baseline.yaml

# Output: runs/baseline_fc_TIMESTAMP/
#   â”œâ”€â”€ config.yaml          # Experiment configuration
#   â”œâ”€â”€ checkpoints/         # best.pth, last.pth
#   â”œâ”€â”€ logs/tensorboard/    # Training metrics
#   â””â”€â”€ metrics.json         # Final results
```

### Evaluate on Test Set

```bash
# Evaluate best model
python scripts/eval.py --run baseline_fc

# Output: Test Accuracy: 97.82%
```

### Monitor Training

```bash
# Start TensorBoard
tensorboard --logdir=runs

# Open browser: http://localhost:6006
```

---

## ğŸ“ Project Structure

```
mnist_classifier/
â”œâ”€â”€ configs/                    # Experiment configurations
â”‚   â”œâ”€â”€ baseline.yaml          # âœ… Baseline FC config
â”‚   â”œâ”€â”€ cnn.yaml              # ğŸ“ TODO: CNN config
â”‚   â””â”€â”€ ablations/            # ğŸ“ TODO: Ablation studies
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ mnist.py          # âœ… Data loading with train/val/test splits
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ fc.py             # âœ… Production fully connected model
â”‚   â”‚   â””â”€â”€ cnn.py            # ğŸ“ TODO: CNN implementation 
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py        # âœ… Main training loop
â”‚   â”‚   â”œâ”€â”€ checkpointing.py  # âœ… Model saving/loading
â”‚   â”‚   â”œâ”€â”€ early_stopping.py # âœ… Early stopping logic
â”‚   â”‚   â””â”€â”€ metrics.py        # âš ï¸ Basic metrics (needs expansion)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py         # âœ… YAML config management
â”‚   â”‚   â”œâ”€â”€ logging.py        # âœ… TensorBoard + JSON logging
â”‚   â”‚   â”œâ”€â”€ seed.py           # âœ… Reproducibility utilities
â”‚   â”‚   â””â”€â”€ device.py         # âœ… GPU/CPU handling
â”‚   â””â”€â”€ legacy/               # âœ… Learning reference implementations
â”‚       â”œâ”€â”€ fc_scratch.py     # From-scratch neural network
â”‚       â””â”€â”€ fc_pytorch.py     # Basic PyTorch implementation
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py              # âœ… Main training script
â”‚   â”œâ”€â”€ eval.py               # âœ… Evaluation script
â”‚   â”œâ”€â”€ visualize_filters.py  # ğŸ“ TODO: Filter visualization
â”‚   â”œâ”€â”€ visualize_failures.py # ğŸ“ TODO: Failure analysis
â”‚   â””â”€â”€ run_ablations.py      # ğŸ“ TODO: Parallel ablations
â”œâ”€â”€ tests/                    # ğŸ“ TODO: Sanity checks
â”‚   â”œâ”€â”€ test_overfit.py
â”‚   â”œâ”€â”€ test_random_labels.py
â”‚   â””â”€â”€ test_single_batch.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_explore_data.ipynb # âœ… Data exploration
â”‚   â””â”€â”€ legacy/               # âœ… Learning notebooks
â”œâ”€â”€ runs/                     # Generated experiment artifacts
â”œâ”€â”€ data/raw/                 # MNIST dataset (auto-downloaded)
â”œâ”€â”€ PROJECT_CONTEXT.md        # âœ… Detailed project state
â”œâ”€â”€ KEY_CODE.md              # âœ… Code patterns reference
â””â”€â”€ requirements.txt         # âœ… Python dependencies
```

**Legend:** âœ… Complete | âš ï¸ Partial | ğŸ“ TODO

---

## ğŸ”§ Configuration System

All experiments are defined via YAML configs in `configs/`. Example:

```yaml
name: baseline_fc
seed: 42

model:
  type: fc
  input_size: 784
  hidden_sizes: [256, 128]
  num_classes: 10
  dropout: 0.2

data:
  dataset: mnist
  train_split: 0.8  # 80% train, 20% val
  batch_size: 64

training:
  epochs: 20
  learning_rate: 0.001
  optimizer: adam
  early_stopping:
    patience: 5
    min_delta: 0.001
```

**Benefits:**
- Reproducible experiments
- Version-controlled hyperparameters
- Easy comparison across runs
- Config saved with each experiment

---

## ğŸ§ª Key Features

### 1. Proper Data Splits
- **Train (80%):** Model training
- **Validation (20%):** Early stopping, checkpoint selection
- **Test (held-out):** Final evaluation only

Prevents the common antipattern of "tuning on test set."

### 2. Production Training Pipeline
- **TensorBoard logging:** Real-time training curves
- **Checkpointing:** Saves best and last models
- **Early stopping:** Prevents overfitting, saves compute
- **Reproducible:** Seed control for deterministic results

### 3. Experiment Tracking
Each training run creates a timestamped directory:
```
runs/baseline_fc_20260122_014543/
â”œâ”€â”€ config.yaml          # Exact config used
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best.pth        # Best validation loss
â”‚   â””â”€â”€ last.pth        # Final epoch
â”œâ”€â”€ logs/tensorboard/   # Training metrics
â””â”€â”€ metrics.json        # Summary statistics
```

### 4. From-Scratch Learning Path
`src/legacy/` contains educational implementations:
- **fc_scratch.py:** Manual backpropagation (91.66% accuracy)
- **fc_pytorch.py:** Basic PyTorch (98.16% accuracy)

These serve as correctness references and demonstrate progression to production code.

---

## ğŸ“š Documentation

- **[PROJECT_CONTEXT.md](PROJECT_CONTEXT.md)** - Complete project state, design decisions, next tasks
- **[KEY_CODE.md](KEY_CODE.md)** - API reference, code patterns, quick commands
- **[requirements.txt](requirements.txt)** - Python dependencies

---

## ğŸ“ Learning Outcomes

### Phase 1: Foundations âœ…
- [x] Data exploration and visualization
- [x] Understanding MNIST format and normalization
- [x] Class distribution analysis

### Phase 2: Neural Networks âœ…
- [x] Forward pass (matrix multiplications)
- [x] Backpropagation (gradient computation)
- [x] Loss functions (cross-entropy)
- [x] Weight initialization (Xavier/Kaiming)
- [x] Activation functions (ReLU, Softmax)

### Phase 3: PyTorch Fundamentals âœ…
- [x] nn.Module architecture
- [x] Automatic differentiation
- [x] DataLoader and transforms
- [x] Optimizers (Adam, SGD)
- [x] GPU/CPU device management

### Phase 4: Production ML Engineering âœ…
- [x] Config-driven experiments
- [x] Proper train/val/test splits
- [x] TensorBoard integration
- [x] Model checkpointing
- [x] Early stopping
- [x] Reproducibility (seeding)

### Phase 5: Advanced Topics (In Progress)
- [ ] Sanity checks (overfit test, random labels)
- [ ] CNN implementation
- [ ] Data augmentation
- [ ] Systematic ablation studies
- [ ] Model interpretability (confusion matrix, failure analysis)
- [ ] Filter visualization

---

## ğŸ”œ Roadmap

### Note on Repository Structure
This repository includes placeholder files (empty or minimal implementations) for features planned in upcoming phases. The directory structure is complete to maintain clean organization as features are added.

**Currently Implemented:**
- Full training pipeline (config â†’ train â†’ checkpoint â†’ eval)
- Baseline FC model with 97.82% test accuracy
- All infrastructure utilities (logging, seeding, device management)

**Next to Implement (files exist as placeholders):**
- CNN model and training
- Interpretability scripts
- Sanity check tests
- Ablation experiments

### Immediate Next Steps
1. **Sanity Checks** - Validate training pipeline
   - Overfit 128 samples test
   - Random labels test
   - Single batch training test

2. **CNN Implementation** - Target 99%+ accuracy
   - Conv2d layers with pooling
   - Batch normalization
   - Filter visualization

3. **Ablation Studies** - Understand what matters
   - Initialization schemes (Xavier vs Kaiming)
   - Dropout impact
   - Learning rate schedules
   - Batch normalization effect

4. **Interpretability** - Debug model decisions
   - Confusion matrix analysis
   - Top-25 confident mistakes
   - Failure case clustering
   - Activation map visualization

### Future Enhancements
- Transfer learning to CIFAR-10
- ResNet architecture
- Distributed training
- Model quantization
- ONNX export

---

## ğŸ¤ Contributing

This is a learning project. Contributions that improve:
- Code clarity and documentation
- Educational value
- Production best practices
- Test coverage

...are welcome!

---

## ğŸ“– Resources

- **PyTorch Documentation:** https://pytorch.org/docs/
- **MNIST Dataset:** http://yann.lecun.com/exdb/mnist/
- **TensorBoard Guide:** https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- Yann LeCun et al. for the MNIST dataset
- PyTorch team for the deep learning framework
- Anthropic Claude for development assistance

---

## ğŸ“§ Contact

**Author:** Abdullah Galib  
**GitHub:** [@galileo-gal](https://github.com/galileo-gal)  
**Repo:** [mnist-classifier](https://github.com/galileo-gal/mnist-classifier)

---

**Last Updated:** January 22, 2026  
**Version:** 0.1 (Baseline Complete)
